{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "baudrillard = '/Users/finchmf/coding/TwitterRitual/core/behavior/text/webscraper/baudrillard/textData/simulacra-and-simulation.txt'\n",
    "baudrillard_2 = '/Users/finchmf/coding/TwitterRitual/core/behavior/text/webscraper/baudrillard/textData/simulations.txt'\n",
    "def load_data(path: str) -> str:\n",
    "\n",
    "    text_file = os.path.join(path)\n",
    "    with open(text_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "_data = load_data(baudrillard)\n",
    "_data_2 = load_data(baudrillard_2)\n",
    "main_data = _data + _data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 14440\n",
      "Number of lines: 11204\n",
      "Average number of words in each line: 7.909139593002499\n",
      "\n",
      "The lines 100 to 150:\n",
      "whose operation is nuclear and genetic, no longer at all specular or discursive. It is all of \n",
      "metaphysics that is lost. No more mirror of being and appearances, of the real and its \n",
      "concept. No more imaginary coextensivity: it is genetic miniaturization that is the \n",
      "dimension of simulation. The real is produced from miniaturized cells, matrices, and \n",
      "memory banks, models of control - and it can be reproduced an indefinite number of \n",
      "times from these. It no longer needs to be rational, because it no longer measures itself \n",
      "against either an ideal or negative instance. It is no longer anything but operational. In \n",
      "fact, it is no longer really the real, because no imaginary envelops it anymore. It is a \n",
      "hyperreal, produced from a radiating synthesis of combinatory models in a hyperspace \n",
      "without atmosphere. \n",
      "\n",
      "By crossing into a space whose curvature is no longer that of the real, nor that of truth, \n",
      "the era of simulation is inaugurated by a liquidation of all referentials - worse: with their \n",
      "\n",
      "\n",
      "artificial resurrection in the systems of signs, a material more malleable than meaning, in \n",
      "that it lends itself to all systems of equivalences, to all binary oppositions, to all \n",
      "combinatory algebra. It is no longer a question of imitation, nor duplication, nor even \n",
      "parody. It is a question of substituting the signs of the real for the real, that is to say of an \n",
      "operation of deterring every real process via its operational double, a programmatic, \n",
      "metastable, perfectly descriptive machine that offers all the signs of the real and short- \n",
      "circuits all its vicissitudes. Never again will the real have the chance to produce itself - \n",
      "such is the vital function of the model in a system of death, or rather of anticipated \n",
      "resurrection, that no longer even gives the event of death a chance. A hyperreal \n",
      "henceforth sheltered from the imaginary, and from any distinction between the real and \n",
      "the imaginary, leaving room only for the orbital recurrence of models and for the \n",
      "simulated generation of differences. \n",
      "\n",
      "THE DIVINE IRREFERENCE OF IMAGES \n",
      "\n",
      "To dissimulate is to pretend not to have what one has. To simulate is to feign to have \n",
      "what one doesn't have. One implies a presence, the other an absence. But it is more \n",
      "complicated than that because simulating is not pretending: \"Whoever fakes an illness \n",
      "can simply stay in bed and make everyone believe he is ill. Whoever simulates an illness \n",
      "produces in himself some of the symptoms\" (Littre). Therefore, pretending, or \n",
      "dissimulating, leaves the principle of reality intact: the difference is always clear, it is \n",
      "simply masked, whereas simulation threatens the difference between the \"true\" and the \n",
      "\"false,\" the \"real\" and the \"imaginary.\" Is the simulator sick or not, given that he \n",
      "produces \"true\" symptoms? Objectively one cannot treat him as being either ill or not ill. \n",
      "Psychology and medicine stop at this point, forestalled by the illness's henceforth \n",
      "undiscoverable truth. For if any symptom can be \"produced,\" and can no longer be taken \n",
      "as a fact of nature, then every illness can be considered as simulatable and simulated, and \n",
      "medicine loses its meaning since it only knows how to treat \"real\" illnesses according to \n",
      "their objective causes. Psychosomatics evolves in a dubious manner at the borders of the \n",
      "principle of illness. As to psychoanalysis, it transfers the symptom of the organic order to \n",
      "the unconscious order: the latter is new and taken for \"real\" more real than the other - but \n",
      "why would simulation be at the gates of the unconscious? Why couldn't the \"work\" of the \n",
      "unconscious be \"produced\" in the same way as any old symptom of classical medicine? \n",
      "Dreams already are. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_line_range = (100, 150)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in main_data.split()})))\n",
    "\n",
    "lines = main_data.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(main_data.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Developed Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data\n",
    "\n",
    "# NOTE: paths are all specific amd hard coded. be explicit\n",
    "\n",
    "# in the case of having multiple txt documents\n",
    "baudrillard = '/Users/finchmf/coding/TwitterRitual/core/behavior/text/webscraper/baudrillard/textData/simulacra-and-simulation.txt'\n",
    "baudrillard_2 = '/Users/finchmf/coding/TwitterRitual/core/behavior/text/webscraper/baudrillard/textData/simulations.txt'\n",
    "text = data.convertManyToOne([baudrillard, baudrillard_2])\n",
    "# otherwise use data.load_data(text_path) for single txt document\n",
    "\n",
    "# set tmp location for runtime generated single txt doc combing all txt documents\n",
    "tmp_path = '/Users/finchmf/coding/TwitterRitual/core/behavior/text/webscraper/baudrillard/textData/tmp'\n",
    "tmp_file = 'tmp_combined.txt'\n",
    "# check and create if needed\n",
    "if not os.path.exists(tmp_path):\n",
    "    os.mkdir(tmp_path)\n",
    "# save\n",
    "with open(f'{tmp_path}/{tmp_file}', 'w') as f:\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# get text title\n",
    "text_tile = 'baudrillad_combined'\n",
    "# check and create if needed\n",
    "if not os.path.exists(f'processedData/{text_title}_preprocess.p'):\n",
    "\n",
    "    data.preprocess_and_save(\n",
    "        text_path=f'{tmp_path}/{tmp_file}',\n",
    "        text_title=text_title\n",
    "    )\n",
    "# load nlp data for modeling\n",
    "int_text, w2i, i2w, token_dict = data.load_preprocess(text_title)\n",
    "\n",
    "CMD = f'rm -r {tmp_path}'\n",
    "os.system(CMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(\n",
    "    words: list, \n",
    "    sequence_length: int, \n",
    "    batch_size: int) -> DataLoader:\n",
    "\n",
    "    words = np.array(words)\n",
    "    total_batch_size = batch_size * sequence_length\n",
    "    n_batches = len(words) // batch_size\n",
    "\n",
    "    words = words[:n_batches * total_batch_size]\n",
    "    feature, target = [], []\n",
    "\n",
    "    target_words_length = words[:-sequence_length]\n",
    "    for idx in range(0, len(target_words_length)):\n",
    "\n",
    "        feature.append(words[idx: idx + sequence_length])\n",
    "        target.append(words[idx + sequence_length])\n",
    "\n",
    "    batch_nums = len(words) // batch_size\n",
    "    feature = feature[:batch_nums * batch_size]\n",
    "    target = target[:batch_nums * batch_size]\n",
    "\n",
    "    feature_tensors = torch.from_numpy(np.asarray(feature))\n",
    "    target_tensors = torch.from_numpy(np.asarray(target))\n",
    "\n",
    "    data_set = TensorDataset(feature_tensors, target_tensors)\n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloader\n",
    "\n",
    "# test_text = range(50)\n",
    "# t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "# data_iter = iter(t_loader)\n",
    "# sample_x, sample_y = data_iter.next()\n",
    "\n",
    "# print(sample_x.shape) # batch_size x sequence_length\n",
    "# print(sample_x) # batch\n",
    "# print()\n",
    "# print(sample_y.shape) # text | len(words)\n",
    "# print(sample_y) # text | list(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  343,     7,     0,    61,   478,     2, 11576],\n",
      "        [   50,     8,     2,    28,  2421,     4, 11651],\n",
      "        [ 3323,    11,  5320,     8,  1134,  2133,    55],\n",
      "        [  455,     1,  8233,  8234,     2,  8235,     5],\n",
      "        [  734,     1,   150,    18,    93,   234,    22],\n",
      "        [   50,     8,     2, 12161,     5,  2427,     9],\n",
      "        [ 9320,     4,   512,     5,     0,   861,  1502],\n",
      "        [15386, 15387,   462,    16,  2602,   132,     0],\n",
      "        [ 7719,   949,     1,   363,     1,   253,    53],\n",
      "        [    7,     3,    40,   103,    38,   111,   315],\n",
      "        [  895,     1,  6387,     0,  6388,     7,     2],\n",
      "        [    1,     0,   929,    66,     4,     1,     0],\n",
      "        [    3,   173,   130,     7,  2628,   132,    25],\n",
      "        [ 6303,     1,     0,  6304,   881,    84,    88],\n",
      "        [   55,    39,     4,   419,  1971,    17,  7755],\n",
      "        [    2,     0,  9636,  9637,    23,    37,   118],\n",
      "        [    0,  2307,     4,  9210,  1801,     1,     0],\n",
      "        [  183,   371,   548,     5, 14035,  2838,     5],\n",
      "        [  157,    37,    12,    29,    24,  8918,  1549],\n",
      "        [   61,     4,    90,  4387,     3,  1812, 13458],\n",
      "        [  537,     1,     0,  1657,     1,     0,   588],\n",
      "        [  906,   168,     1,    95,     2,  2912,    87],\n",
      "        [   50,     8,    59, 10790,     8,     2,   271],\n",
      "        [ 6800,    59,  6801,    10,     3,  6802,     5],\n",
      "        [   13,   332, 12844,    69,   137,     1,    64],\n",
      "        [ 9258,  5165,   305,  9259,  1303,   666,  2932],\n",
      "        [  601,  1104,     0,   162,     1,  1511,    11],\n",
      "        [    5,  1394,    50,    71,    88,  5885,     8],\n",
      "        [  628,    79,   560,    53,  4325,    79,   263],\n",
      "        [    5,  2643,     7,     8,     2,     0,    61],\n",
      "        [   12,    29,     3,   121,     1,     0,   455],\n",
      "        [    7,  4278,   303,  6954,  2145,     1,     0]])\n",
      "tensor([    4,     6,     8,    14,  6902,   198,  5180,  1873,   796,   299,\n",
      "            5,   930, 12701,   720,    25,     7,  2338,    14,     0,   204,\n",
      "            1,  4432,     0,   781, 12845,  5166,    46,     2,     4,     6,\n",
      "            1,   158])\n"
     ]
    }
   ],
   "source": [
    "n_loader = batch_data(int_text, sequence_length=7, batch_size=32)\n",
    "data_iter = iter(n_loader)\n",
    "x,y = data_iter.next()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def forward_back_prop(\n",
    "    model, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    inp, \n",
    "    target, \n",
    "    hidden,\n",
    "    train_on_gpu=False):\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "\n",
    "    model.zero_grad()\n",
    "    output, hidden = model(inp, hidden)\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    clip = 5\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), hidden\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(\n",
    "    model,\n",
    "    batch_size,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    n_epochs,\n",
    "    train_loader,\n",
    "    show_every_n_batches=100):\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f' | Training for {n_epochs} epoch(s).....')\n",
    "    for e in range(1, n_epochs + 1):\n",
    "\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader, 1):\n",
    "\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "\n",
    "            if batch_idx > n_batches:\n",
    "                break\n",
    "\n",
    "            loss, hidden = forward_back_prop(\n",
    "                                model=model,\n",
    "                                optimizer=optimizer,\n",
    "                                criterion=criterion,\n",
    "                                inp=inputs,\n",
    "                                target=labels,\n",
    "                                hidden=hidden\n",
    "            )\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            if batch_idx % show_every_n_batches == 0:\n",
    "                print(f' | EPOCH: {e:>4}/{n_epochs:>4} -- LOSS: {np.average(batch_losses)}')\n",
    "                batch_losses = []\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "learning_rate = 0.0001\n",
    "vocab_size = len(w2i)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 3\n",
    "sequence_length = 200\n",
    "batch_size = 32\n",
    "show_every_n_batches = 500\n",
    "\n",
    "model = LSTM(\n",
    "        vocab_size=vocab_size,\n",
    "        output_size=output_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_layers=n_layers\n",
    ")\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Training for 4 epoch(s).....\n",
      " | EPOCH:    1/   4 -- LOSS: 9.327696741104125\n",
      " | EPOCH:    1/   4 -- LOSS: 9.21197619819641\n",
      " | EPOCH:    1/   4 -- LOSS: 9.176715986251832\n",
      " | EPOCH:    1/   4 -- LOSS: 9.170046142578125\n",
      " | EPOCH:    1/   4 -- LOSS: 9.153282485961913\n",
      " | EPOCH:    2/   4 -- LOSS: 9.145766514950518\n",
      " | EPOCH:    2/   4 -- LOSS: 9.13773410987854\n",
      " | EPOCH:    2/   4 -- LOSS: 9.130745916366577\n",
      " | EPOCH:    2/   4 -- LOSS: 9.132054363250733\n",
      " | EPOCH:    2/   4 -- LOSS: 9.125159162521362\n",
      " | EPOCH:    3/   4 -- LOSS: 9.123021045183794\n",
      " | EPOCH:    3/   4 -- LOSS: 9.127365449905396\n",
      " | EPOCH:    3/   4 -- LOSS: 9.118731203079223\n",
      " | EPOCH:    3/   4 -- LOSS: 9.118500480651855\n",
      " | EPOCH:    3/   4 -- LOSS: 9.124070850372314\n",
      " | EPOCH:    4/   4 -- LOSS: 9.120307922363281\n",
      " | EPOCH:    4/   4 -- LOSS: 9.11461480331421\n",
      " | EPOCH:    4/   4 -- LOSS: 9.111514724731446\n",
      " | EPOCH:    4/   4 -- LOSS: 9.115749227523803\n",
      " | EPOCH:    4/   4 -- LOSS: 9.109112625122071\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_LSTM(\n",
    "                    model=model,\n",
    "                    batch_size=batch_size,\n",
    "                    optimizer=optimizer,\n",
    "                    criterion=criterion,\n",
    "                    n_epochs=num_epochs,\n",
    "                    train_loader=n_loader,\n",
    "                    show_every_n_batches=show_every_n_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save_model(filename='models/baudrillard_v1.pt', decoder=trained_model)\n",
    "# data.load_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    prime_id,\n",
    "    i2w,\n",
    "    token_dict,\n",
    "    pad_value,\n",
    "    predict_len=100\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [i2w[prime_id]]\n",
    "\n",
    "    for _ in range(predict_len):\n",
    "\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "\n",
    "        hidden = model.init_hidden(current_seq.size(0))\n",
    "\n",
    "        output, _ = model(current_seq, hidden)\n",
    "\n",
    "        p = F.softmax(output, dim=1).data\n",
    "\n",
    "        if train_on_gpu:\n",
    "            p = p.cpu()\n",
    "\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "\n",
    "        word = i2w[word_i]\n",
    "        predicted.append(word)\n",
    "\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "\n",
    "    gen_sentences = ' '.join(predicted)\n",
    "\n",
    "    for k, v in token_dict.items():\n",
    "\n",
    "        ending = ' ' if k in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + v.lower(), k)\n",
    "\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "\n",
    "    return gen_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triumph of is is the is a of to to is is a the to is a to to is is to is to to is the the the the a of a a the the is a is to the is of is is of is a of of a a a of the to the the a of the a of to the of of the the the of of of a is is of the is a of the the is to is is of the a to the of the a of to the the of a to is of the to to a to of the the of to the to a to the to of is is of the the of is to is of the the the a of of is the to of of the to is the of a is the is to of to a is is the to a to a a of a a a is to to a the the the the the of the the a is a to the the a to to to the of the is a to the is a is the the the to is a the of of to is to of is to is a of to to to of of of to is a of a of to a the of a to the to of of the a of a to the a the is is a a to to of is to is of to of to of of of is the the is the a is of to the to to to is to the of a is is of to a the the a is of to the is the of is of to of to of a of of the is to is the the a of to is the the the a of a of is is a the a the the the is to is the is a a is is to is a to of is a a is a is a of to to a is is is to of to a is of a of of the is of of to a is of of is the to is to of of of to of to to a of the a to of to to to a of a\n"
     ]
    }
   ],
   "source": [
    "gen_length = 400\n",
    "prime_word = 'triumph'\n",
    "\n",
    "pad_word = data.SPECIAL_WORDS['PADDING']\n",
    "generated_text = generate(\n",
    "\n",
    "                    model=trained_model,\n",
    "                    prime_id=w2i[prime_word],\n",
    "                    i2w=i2w,\n",
    "                    token_dict=token_dict,\n",
    "                    pad_value=w2i[pad_word],\n",
    "                    predict_len=gen_length\n",
    ")\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baudrilliardv1_generated_text.txt', 'w') as handle:\n",
    "\n",
    "    handle.write(generated_text)\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2821f07e8631336d86e8598c0ef80e1a73ee1b0e02af0ddc389e7a58f7ae87f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
